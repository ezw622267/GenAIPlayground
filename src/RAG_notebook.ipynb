{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956f57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08e40926",
   "metadata": {},
   "source": [
    "## RAG notebook\n",
    "#### This notebook will go over basic RAG examples\n",
    "\n",
    "#### What is RAG? \n",
    "\n",
    "LLMs excel at producing answers based on \"general\" knowledge (this is the data they were originally trained on). When posing questions to an LLM based on niche data, or user-specific data, they will usually fail to produce correct or relevant answers. \n",
    "\n",
    "We want to somehow add background knowledge to our question so the LLM knows what to do with it. Here is the example\n",
    "\n",
    "query = 'Summarize the history of Virginia' \n",
    "\n",
    "^ this query might be okay - it is possible the model has seen enough information about Virginia to answer, but maybe not! \n",
    "\n",
    "better_query = 'Summarize the history of Virginia + [state creation, state constitution, key figures etc.] \n",
    "\n",
    "the content in the brackets is fetched from somewhere. \n",
    "\n",
    "1) How do we effectively store data? \n",
    "2) How do we know what is \"relevant\" data?\n",
    "3) How much context is \"enough\"\n",
    "\n",
    "^ these are some of the key questions that engineers and researchers ask themselves when studying and building RAG systems\n",
    "\n",
    "#### Context Window Trade-off\n",
    "\n",
    "Why can't we just paste in the relevant data directly into our query without dealing with storage and retrieval? This is because LLMs have limited \"context windows\". \n",
    "\n",
    "\"Context windows\" are how many tokens the LLM can process at once. This is like the working memory of an LLM. A helpful analogy is person A gives someone digits of pi and then asks person B to recall the digits in forward and backwards order. The number of digits they can do this without error is essentially a context window. \n",
    "\n",
    "If LLMs had infinite context windows, we wouldn't need RAG. There are pros and cons. \n",
    "\n",
    "1) Long Context Windows - consistently outperform RAG, but are more expensive (when using free models, this means it takes more time to analyze). \n",
    "\n",
    "2) RAG is efficient - you only supply the most relevant information and is faster. \n",
    "\n",
    "As with many concepts in software, there are performance vs. efficiency tradeoffs. Ideally, it helps to leverage both *together* when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d395031",
   "metadata": {},
   "source": [
    "## Example 1 - Basic Query vs. Augmented Query\n",
    "\n",
    "#### The example will show the differences about when we ask a query about a topic that an LLM may or may not have enough background information on w/ and w/o context. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54e332",
   "metadata": {},
   "source": [
    "#### Early History of VA (general knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea113440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama \n",
    "general_query = 'give me a three paragraph summary of early Virgina history'\n",
    "\n",
    "response = ollama.generate(model = 'gemma2:2b',\n",
    "                           prompt = general_query)['response']\n",
    "#let's write the response to a text file for easy reading \n",
    "\n",
    "f = open('llm_va_history_response.txt','w')\n",
    "f.write(response)\n",
    "f.close() #make sure to close the file\n",
    "\n",
    "#check your project directory for the generated text file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f92bab",
   "metadata": {},
   "source": [
    "### Early History of VA (with context)\n",
    "\n",
    "#### paragraphs taken from: https://www.britannica.com/place/Virginia-state/Independence-and-statehood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5cd8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_va_history = open('early_va_history.txt').read() \n",
    "\n",
    "query_with_context = f\"{general_query}. Use this text as context: {early_va_history}\"\n",
    "\n",
    "response_with_context = ollama.generate(model = 'gemma2:2b',\n",
    "                                        prompt = query_with_context)['response']\n",
    "\n",
    "f = open('llm_response_with_context.txt','w')\n",
    "f.write(response_with_context)\n",
    "f.close() \n",
    "\n",
    "#again, check your project directory \n",
    "#compare answers to the generated files \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f4a60",
   "metadata": {},
   "source": [
    "#### Note here that Gemma2-2b's context window was long enough such that we could just add the contents of the text file to the query. This is an ideal scenario.\n",
    "\n",
    "#### With RAG, you want to break up the text into smaller chunks and store it. This process is called \"indexing\". \n",
    "\n",
    "#### Recall, that LLMs, and all other ML models, convert text into vectors (technically \"tensors\", but not necessary to understand how those work here). These vectors represent the text in a sensible format that is learned during training. (How LLMs are trained will be in a different notebook)\n",
    "\n",
    "#### First, we compute embeddings for our documents. Documents don't strictly have to be text. \"Documents\" in RAG mean some source data that we want to index. They can be images and audio. \n",
    "\n",
    "#### Embeddings are learned in a way that similar words, sentences etc. produce \"similar\" vectors. This means that their \"distance\" from each other is close. Indexing documents is a way to pair embeddings with the documents they came from so when we do find similar vectors, we know what documents they are associated with. How we pair is a design decision based on application! \n",
    "\n",
    "#### When we pose a query to an LLM and we want to fetch context, we want to fetch documents which are relevant to the query. This means we need to embed the query and compare to documents we have indexed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689503df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create embeddings from a file \n",
    "def make_embeddings(file,\n",
    "                    model = 'all-minilm:33m'):\n",
    "    \n",
    "    \n",
    "    lines = open(file,'r').read().split('.')\n",
    "    embedding_dict = {} #dictionary for embeddings\n",
    "    for i,line in enumerate(lines):\n",
    "        '''\n",
    "        For each line, we are going to compute the embedding\n",
    "        each line is a context chunk that we will compare\n",
    "        a query to\n",
    "        '''\n",
    "        embedding = ollama.embeddings(model = model,\n",
    "                                      prompt = line)\n",
    "        \n",
    "        embedding_dict[line] = embedding['embedding']\n",
    "   \n",
    "    return embedding_dict \n",
    "\n",
    "file = 'early_va_history.txt'\n",
    "model = 'all-minilm:33m'\n",
    "#ollama.pull(model)\n",
    "embedding_dict = make_embeddings(file = file,model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a985f",
   "metadata": {},
   "source": [
    "#### Now let's compute the embedding distances between a query and the chunks. We will rank the chunks from most similar (i.e. smallest distance) to least similar (greatest distance). To do this efficiently, we will use a data structure called a \"heap\" to track the top-k most relevant documents\n",
    "\n",
    "#### We don't *have* to use heaps since our text file is small, but as the number of documents increase - we need to store the most relevant ones efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f38864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 (distance: 4.6651): The original inhabitants of Virginia arrived some 10,000 to 12,000 years ago\n",
      "chunk 2 (distance: 5.6623):  These were people of Paleo-Indian culture, who, like their successors, the Archaic-culture people, lived mainly by hunting and fishing\n",
      "chunk 3 (distance: 4.6479):  The coastal areas of eastern Virginia supported a significant population of indigenous peoples who fished in the rivers and bays and hunted wild fowl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import heapq \n",
    "query = \"who were the earliest people in modern day Virginia?\"\n",
    "top_k = 3 #top 3 chunks, the document has 12 sentences so pick a number 1 <= top_k <= 11 \n",
    "\n",
    "def compute_distances(query,embedding_dict):\n",
    "    query_embedding = np.array(ollama.embeddings(model = model,\n",
    "                                        prompt = query)['embedding'])\n",
    "    distance_dict = {}\n",
    "    for chunk,_embedding in embedding_dict.items():\n",
    "        embedding = np.array(_embedding)\n",
    "        distance = np.linalg.norm(embedding - query_embedding)\n",
    "        distance_dict[chunk] = distance.item()\n",
    "    \n",
    "    top_k_chunks = heapq.nlargest(top_k,distance_dict)\n",
    "    return {'chunks': top_k_chunks,\n",
    "            'distances': distance_dict}\n",
    "\n",
    "distance_dict = compute_distances(query,embedding_dict)\n",
    "#let's now see the top chunks (not necessarily sorted)\n",
    "for i,chunk in enumerate(distance_dict['chunks']):\n",
    "    chunk_query_dist = distance_dict['distances'][chunk]\n",
    "    print(f\"chunk {i+1} (distance: {chunk_query_dist:.4f}): {chunk}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4fafa",
   "metadata": {},
   "source": [
    "#### GraphRAG example coming soon "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
